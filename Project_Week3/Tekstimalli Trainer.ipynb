{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'  # You can change to 'jax' or 'torch' if preferred\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sentencepiece as spm\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras backend: {keras.config.backend()}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and prepare text data\n",
    "# Load the Chosen text\n",
    "with open('./kalevala_puhdas.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    text = file.read()#.lower()\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"First 100 characters: {text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Train SentencePiece model\n",
    "# Save text to a temporary file for SentencePiece training\n",
    "temp_file = 'kalevala_temp.txt'\n",
    "with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Train SentencePiece model\n",
    "vocab_size = 40000  # You can adjust this based on your needs\n",
    "model_prefix = 'kalevala_sp'\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=temp_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=1.0,  # Important for Finnish\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['<PAD>', '<UNK>']\n",
    ")\n",
    "\n",
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{model_prefix}.model\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Vaka vanha Väinämöinen\"\n",
    "tokens = sp.encode_as_pieces(test_text)\n",
    "print(f\"Tokenized example: {tokens}\")\n",
    "print(f\"Vocabulary size: {sp.get_piece_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ae8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare training data\n",
    "# Tokenize the text\n",
    "seq_length = 64\n",
    "pieces = sp.encode_as_ids(text)\n",
    "print(f\"Total tokens: {len(pieces)}\")\n",
    "\n",
    "# Create sequences\n",
    "sequences = []\n",
    "for i in range(0, len(pieces) - seq_length):\n",
    "    # Input: first seq_length tokens, Target: next seq_length tokens (shifted by 1)\n",
    "    sequences.append(pieces[i:i+seq_length+1])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "sequences = np.array(sequences)\n",
    "inputs = sequences[:, :-1]  # All tokens except the last one\n",
    "targets = sequences[:, 1:]  # All tokens except the first one\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "indices = np.arange(len(sequences))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_inputs, train_targets = inputs[train_indices], targets[train_indices]\n",
    "val_inputs, val_targets = inputs[val_indices], targets[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26427cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define the model\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
    "    positions = np.arange(max_len)[:, np.newaxis]\n",
    "    angles = np.arange(d_model)[np.newaxis, :] / d_model\n",
    "    angles = 1 / (10000**angles)\n",
    "\n",
    "    pos_encoding = positions * angles\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "# Define model parameters\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 3\n",
    "\n",
    "# Create the model\n",
    "inputs = keras.Input(shape=(seq_length,))\n",
    "embedding_layer = keras.layers.Embedding(sp.get_piece_size(), embed_dim)(inputs)\n",
    "\n",
    "# Add positional encoding\n",
    "pos_encoding = get_positional_encoding(seq_length, embed_dim)\n",
    "x = embedding_layer + pos_encoding\n",
    "\n",
    "# Helper function to create causal attention mask\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"Create a causal attention mask to prevent looking at future tokens.\"\"\"\n",
    "    mask = 1 - np.triu(np.ones((size, size)), k=1)\n",
    "    return mask  # Lower triangular matrix\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_layers):\n",
    "    # Multi-head attention with causal mask\n",
    "    # Manually create causal mask since use_causal_mask parameter isn't available\n",
    "    causal_mask = create_causal_mask(seq_length)\n",
    "\n",
    "    # Apply attention with manual causal mask\n",
    "    attention_output = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads\n",
    "    )(x, x, attention_mask=causal_mask)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = keras.Sequential([\n",
    "        keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        keras.layers.Dense(embed_dim),\n",
    "        keras.layers.Dropout(0.1)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "# Final output layer\n",
    "outputs = keras.layers.Dense(sp.get_piece_size())(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train the model\n",
    "batch_size = 64\n",
    "epochs = 20 \n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, train_targets,\n",
    "    validation_data=(val_inputs, val_targets),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1),\n",
    "        keras.callbacks.ModelCheckpoint('kalevala_best_model.keras', save_best_only=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Cell 6.5: Save the model\n",
    "model.save('kalevala_model.keras')\n",
    "print(\"Model saved as 'kalevala_model.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd148df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot training metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
