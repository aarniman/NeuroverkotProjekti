{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfdf5fa-b72c-41e2-9f58-fe30746d2f87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bfdf5fa-b72c-41e2-9f58-fe30746d2f87",
    "outputId": "c72217e2-f202-4e9f-88aa-61020cc930a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.9.2\n",
      "Keras backend: tensorflow\n",
      "Tensorflow version: 2.19.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'  # You can change to 'jax' or 'torch' if preferred\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sentencepiece as spm\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras backend: {keras.config.backend()}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945eb0c",
   "metadata": {},
   "source": [
    "### Ladataan malli ja generoidaan teksti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11622b",
   "metadata": {},
   "source": [
    "Ladataan malli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d49d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "model = keras.models.load_model('kalevala_model.keras')\n",
    "# Load SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('kalevala_sp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62eb512",
   "metadata": {},
   "source": [
    "Tekstin generointifunktio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a998c8d3-bb56-4f56-b5a2-cac6275fd26f",
   "metadata": {
    "id": "a998c8d3-bb56-4f56-b5a2-cac6275fd26f"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, sp, prompt, num_tokens=100, temperature=1.0):\n",
    "    \"\"\"Generate text based on a prompt with proper lowercase handling.\"\"\"\n",
    "    # Convert prompt to lowercase to match training data\n",
    "    lowercase_prompt = prompt#.lower()\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = sp.encode_as_ids(lowercase_prompt)\n",
    "\n",
    "    # Rest of your generation code stays the same...\n",
    "    if len(input_ids) < seq_length:\n",
    "        padding_length = seq_length - len(input_ids)\n",
    "        input_ids = [0] * padding_length + input_ids\n",
    "    else:\n",
    "        padding_length = 0\n",
    "        input_ids = input_ids[-seq_length:]\n",
    "\n",
    "    # Generated tokens\n",
    "    generated_ids = list(input_ids[padding_length:])\n",
    "\n",
    "    # Generate text token by token\n",
    "    for _ in range(num_tokens):\n",
    "        x = np.array([input_ids])\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        logits = predictions[-1]\n",
    "        logits = logits / temperature\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        next_token = np.random.choice(len(probs), p=probs)\n",
    "        generated_ids.append(next_token)\n",
    "        input_ids = input_ids[1:] + [next_token]\n",
    "\n",
    "    # Decode the generated sequence\n",
    "    generated_text = sp.decode(generated_ids)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c18b5",
   "metadata": {},
   "source": [
    "Generoidaan hieman tekstiä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9580493-d3a5-4cfc-9943-fe99dcb8e01d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9580493-d3a5-4cfc-9943-fe99dcb8e01d",
    "outputId": "3489ff49-3d77-4f36-f73a-65a9a47324b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Vaka vanha Väinämöinen\n",
      "Vaka vanha Väinämöinen, tietäjä iän-ikuinen; virkki nuori tulisen, väljiä vesiä! \"Suvetar, valio vaimo, Etelätär, luonnon eukko! Käy nyt, syötä Syötikkini sekä juota Juotikkini, herustele Hermikkiä, tuorustele Tuorikkia, anna maito Mairikille, Omenalle uuet piimät hele'istä heinänpäistä, kaunihista kastikoista, mairehista maaemistä, metisistä mättähistä, nurmelta mesinukalta, maalta marjanvartiselta, kanervan-kukattarilta, heinän-helpehettäriltä, pilven piimätyttäriltä, taivahan-navattarilta, tuoa maitoiset maruet, aina uhkuvat utaret lypseä lyhyen vaimon, pienen piian piukutella! \"Nouse,\n",
      "\n",
      "Prompt: Mieleni minun tekevi\n",
      "Mieleni minun tekevi juomahan soan olutta, soan mettä maistamahan.\" Tuon emo sanoiksi virkki: \"Oi on Ahti poikaseni! Ellös lähtekö sotahan! On meillä oloista koissa leppäisessä lekkerissä tapin tammisen takana; tuon sinulle juoaksesi, josp' on joisit kaiken päivän.\" Sanoi lieto Lemminkäinen: \"En huoli koto-oloista! Ennen juon joesta vettä melan tervaisen terältä: makeamp' on juoakseni, kuin kaikki kotoiset launihin uuen linnan. Yksi kukkui: \"lemmen, lemmen!\" Sanoi vanha Väinämöinen: \"Kuoli ehtoinen emosi, kaatui maire maammuesi. Käypäs tuota katsomahan,\n",
      "\n",
      "Prompt: Lemminkäinen\n",
      "Lemminkäinen. Laski leuan liettehessä, parran paikassa pahassa, suun on suossa, sammalissa, hampahin haon perässä. Sanoi nuori Joukahainen: \"Oi on viisas Väinämöinen, tietäjä iän-ikuinen! Laula jo laulusi takaisin, heitä vielä heikko henki, laske täältä pois minua! Virta jo jalkoa vetävi, hiekka silmiä hiovi. \"Kun pyörrät pyhät sanasi, luovuttelet luottehesi, annan Aino siskoseni, lainoan emoni lapsen sulle pirtin pyyhkijäksi, lattian lakaisijaksi, hulikkojen huuhtojaksi, vaippojen viruttajaksi, kutojaksi kultavaipan, mesileivän leipojaksi.\" Siitä vanha Väinämöinen ihastui ikihyväksi, kun\n",
      "\n",
      "Prompt: Pohjan neito\n",
      "Pohjan neito, pieksän petkelen periksi, huhmaren sukuksi survon. \"Enkä huoli huitukoille, huitukoille, haitukoille; mie tahon tasaisen varren tasaiselle varrelleni, tahon muo'on muhkeamman muhke'ille muo'oilleni sekä kasvon kaunihimman kaunihille kasvoilleni.\" Oli aikoa vähäisen, kului tuskin puoli kuuta. Jo päivänä muutamana, iltana moniahana neitoset kisaelevi, kaunokaiset karkelevi mannerpuolella saloa kaunihilla kankahalla; Kyllikki ylinnä muita, Saaren kukka kuuluisinna. Tuli veitikkä verevä, ajoi lieto Lemminkäinen orihillansa omalla, valitulla varsallansa keskelle kisaketoa, kaunokaisten karkeloa; reutoi Kyllikin rekehen, koppoi neien korjahansa\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Generate sample text\n",
    "prompts = [\n",
    "    \"Vaka vanha Väinämöinen\",\n",
    "    \"Mieleni minun tekevi\",\n",
    "    \"Lemminkäinen\",\n",
    "    \"Pohjan neito\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    generated = generate_text(model, sp, prompt, num_tokens=100, temperature=1.2)\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a2259-373c-4fed-a690-8e96496e04fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "de1a2259-373c-4fed-a690-8e96496e04fc",
    "outputId": "aaf73c89-7abe-424d-84b8-e6d8b1ec11cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# Load saved model\\nmodel = keras.models.load_model(\\'kalevala_model.keras\\')\\n\\n# Load SentencePiece tokenizer\\nsp = spm.SentencePieceProcessor()\\nsp.load(\\'kalevala_sp.model\\')\\n\\n# Test generation\\nprompt = \"Mieleni minun tekevi\"\\ngenerated = generate_text(model, sp, prompt, num_tokens=150, temperature=1.0)\\nprint(generated)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11: Load model (if you're starting a new session)\n",
    "# Uncomment these lines to load a previously saved model\n",
    "\"\"\"\n",
    "# Load saved model\n",
    "model = keras.models.load_model('kalevala_model.keras')\n",
    "\n",
    "# Load SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('kalevala_sp.model')\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Mieleni minun tekevi\"\n",
    "generated = generate_text(model, sp, prompt, num_tokens=150, temperature=1.0)\n",
    "print(generated)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NEURO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
